{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "### Due Date: 03/31/25\n",
    "- Using the knowledge gained from the lecture and the reading, complete the following tasks in Python. Ensure you have spaCy installed and the en_core_web_sm language model downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Installing the spaCy library\n",
    "!pip install spacy\n",
    "\n",
    "# Installing the small English language model used by spaCy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Tokenization\n",
    "### 1. Write a Python script to tokenize the following text: \n",
    "    \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "text = (\"\"\"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\"\")\n",
    "\n",
    "import spacy\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the NLP pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens\n",
    "tokens= [token.text for token in doc] #this puts it in a list \n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Questions to Answer in Comments:\n",
    "\n",
    "**1. How does spaCy process the various tokens?**\n",
    "    Hint: Loop through the doc container using the token attributes: .text_, .head, .lemma_, .morph.\n",
    "- The tokenization process  segments a sentence into words, punctuations, etc. A collection of tokens builds a doc. It processes each token by applying rules specific to each language. The first step is splitting the text based on white space (similar to split), and then the tokenizer performs two checks: 1. Are there any exceptions (ex: despite not having white space doesn't needs to be split), and 2. Can a prefix, suffix of infix be split off.\n",
    "\n",
    "**2. How does spaCy handle punctuation marks like periods and commas?**\n",
    "- Punctuation marks are treated as separte tokens.\n",
    "\n",
    "**3. What happens when the text includes contractions (e.g., \"don't\")?**\n",
    "- SpaCy splits splits tetx in such a way where each token is in its simplest form. For this reason contractions will be split into 2 words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Part-of-Speech Tagging\n",
    "### Extend your script to include part-of-speech tagging for the tokens.\n",
    "    Hint: Use token.pos_ and token.tag_ properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - the - POS: DET, Tag: DT\n",
      "quick - quick - POS: ADJ, Tag: JJ\n",
      "brown - brown - POS: ADJ, Tag: JJ\n",
      "fox - fox - POS: NOUN, Tag: NN\n",
      "does - do - POS: AUX, Tag: VBZ\n",
      "n't - not - POS: PART, Tag: RB\n",
      "jump - jump - POS: VERB, Tag: VB\n",
      "over - over - POS: ADP, Tag: IN\n",
      "the - the - POS: DET, Tag: DT\n",
      "lazy - lazy - POS: ADJ, Tag: JJ\n",
      "dog - dog - POS: NOUN, Tag: NN\n",
      ". - . - POS: PUNCT, Tag: .\n",
      "Natural - Natural - POS: PROPN, Tag: NNP\n",
      "Language - Language - POS: PROPN, Tag: NNP\n",
      "Processing - processing - POS: NOUN, Tag: NN\n",
      "is - be - POS: AUX, Tag: VBZ\n",
      "fascinating - fascinating - POS: ADJ, Tag: JJ\n",
      "! - ! - POS: PUNCT, Tag: .\n"
     ]
    }
   ],
   "source": [
    "#Part of speeach tagging added to tokens\n",
    "for token in doc: #for loop will go through each token and asign a text, lemma, pos_ and tag_ feature to each\n",
    "    print(f\"{token.text} - {token.lemma_} - POS: {token.pos_}, Tag: {token.tag_}\") \n",
    "    \n",
    "#POS Features\n",
    "#token.text - shows original token\n",
    "#token.lemma_ - shows simplest version of word (root form)\n",
    "#token.pos_ - shows the core Universal Part Of Speech (UPOS) category of the word (noun, verb, adjective)\n",
    "#token.tag_ - Fine grained tag (more specific part of category defined in token.pos_)\n",
    "    # ex: token.text = walked, token.pos_: verb, token.tag_: past tense verb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "1. **Identify the POS tags for \"quick,\" \"jumps,\" and \"is.\"**\n",
    "- \"quick\" POS tag: ADJ (adjective)\n",
    "- \"jumps\" POS tag: VERB\n",
    "- \"is\" POS tag: AUX (auxiliary verb)\n",
    "\n",
    "2. **Why might POS tagging be useful for tasks like grammar checking or machine translation?**\n",
    "- One of the main features of POS tagging is that it can identify the type of a word (ex: noun, verb, adjective) and therefore its role in sentence to predict what may be coming next. For example, a machine learning model, might be able to predict that after an adjective a noun is likely. Other features like token.is_stop, can help identify the tokens in a doc that are hold information and therefore are more valauble in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Named Entity Recognition (NER)\n",
    "### Modify your script to identify named entities in the following text:\n",
    "    \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama: PERSON (People, including fictional)\n",
      "44th: ORDINAL (\"first\", \"second\", etc.)\n",
      "the United States: GPE (Countries, cities, states)\n",
      "Hawaii: GPE (Countries, cities, states)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    44th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". He was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hawaii\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = (\"\"\"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\"\")\n",
    "\n",
    "#tokenize\n",
    "doc = nlp(text)\n",
    "tokens= [token.text for token in doc] #this puts it in a list \n",
    "\n",
    "# Display named entities in the text\n",
    "# doc.ents contains only the named entities This ensures we are not looping through all tokens, just the recognized entities.\n",
    "for ent in doc.ents: \n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\") # Print each entity's text, label, and explanation of the label\n",
    "\n",
    "#Better Presentation using Displacy\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True) #style='ent' will highlight entities, suing a differnet color for each category (ex: purple for person, white for ordinal, yellow for GPE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "\n",
    "1. **Which entities are recognized by spaCy?**\n",
    "   Hint: Loop through doc.ents \n",
    "- Barack Obama, 44th, the United States, Hawaii\n",
    "\n",
    "2. **What entity types are assigned to \"Barack Obama\" and \"Hawaii\"?**\n",
    "    Hint: Use token.label_ properties\n",
    "- Barack Obama: PERSON (People, including fictional)\n",
    "- Hawaii: GPE (Geopolitical entity: countries, cities, states, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Experimentation\n",
    "### Write a new sentence or paragraph of your choice and run the spaCy pipeline on it.\n",
    "### Experiment with changing words, adding punctuation, or introducing typos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite - despite - POS: SCONJ, Tag: SCONJ, Dependency: prep\n",
      "loving - love - POS: VERB, Tag: VERB, Dependency: pcomp\n",
      "my - my - POS: PRON, Tag: PRON, Dependency: poss\n",
      "semester - semester - POS: NOUN, Tag: NOUN, Dependency: dobj\n",
      "abroad - abroad - POS: ADV, Tag: ADV, Dependency: advmod\n",
      "in - in - POS: ADP, Tag: ADP, Dependency: prep\n",
      "London - London - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      "and - and - POS: CCONJ, Tag: CCONJ, Dependency: cc\n",
      "the - the - POS: DET, Tag: DET, Dependency: det\n",
      "opportunity - opportunity - POS: NOUN, Tag: NOUN, Dependency: conj\n",
      "to - to - POS: PART, Tag: PART, Dependency: aux\n",
      "travel - travel - POS: VERB, Tag: VERB, Dependency: acl\n",
      "around - around - POS: ADP, Tag: ADP, Dependency: prep\n",
      "Europe - Europe - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      "I - I - POS: PRON, Tag: PRON, Dependency: nsubj\n",
      "really - really - POS: ADV, Tag: ADV, Dependency: advmod\n",
      "missed - miss - POS: VERB, Tag: VERB, Dependency: ROOT\n",
      "Football - Football - POS: PROPN, Tag: PROPN, Dependency: compound\n",
      "season - season - POS: NOUN, Tag: NOUN, Dependency: dobj\n",
      "at - at - POS: ADP, Tag: ADP, Dependency: prep\n",
      "Notre - Notre - POS: PROPN, Tag: PROPN, Dependency: compound\n",
      "Dame - Dame - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      "London: GPE (Countries, cities, states)\n",
      "Europe: LOC (Non-GPE locations, mountain ranges, bodies of water)\n",
      "Notre Dame: FAC (Buildings, airports, highways, bridges, etc.)\n"
     ]
    }
   ],
   "source": [
    "#Text with no errors\n",
    "text = \"Despite loving my semester abroad in London and the opportunity to travel around Europe I really missed Football season at Notre Dame\"\n",
    "doc = nlp(text)\n",
    "\n",
    "#display POS features of text\n",
    "#added POS Feature: .token.dep_ shows the syntatic relationship, meaning the relationship between tokens. This was added as chnages made in the specific chnage made between text and text1 (specificaly loving misspeling) might trigger change in this feature\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_} - POS: {token.pos_}, Tag: {token.pos_}, Dependency: {token.dep_}\")\n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite - despite - POS: SCONJ, Tag: SCONJ, Dependency: prep\n",
      "lving - lve - POS: VERB, Tag: VERB, Dependency: pcomp\n",
      "my - my - POS: PRON, Tag: PRON, Dependency: poss\n",
      "time - time - POS: NOUN, Tag: NOUN, Dependency: dobj\n",
      "abroad - abroad - POS: ADV, Tag: ADV, Dependency: advmod\n",
      "in - in - POS: ADP, Tag: ADP, Dependency: prep\n",
      "London - London - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      "and - and - POS: CCONJ, Tag: CCONJ, Dependency: cc\n",
      "the - the - POS: DET, Tag: DET, Dependency: det\n",
      "opportunity - opportunity - POS: NOUN, Tag: NOUN, Dependency: conj\n",
      "to - to - POS: PART, Tag: PART, Dependency: aux\n",
      "travel - travel - POS: VERB, Tag: VERB, Dependency: acl\n",
      "around - around - POS: ADP, Tag: ADP, Dependency: prep\n",
      "Europe - Europe - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      ", - , - POS: PUNCT, Tag: PUNCT, Dependency: punct\n",
      "I - I - POS: PRON, Tag: PRON, Dependency: nsubj\n",
      "really - really - POS: ADV, Tag: ADV, Dependency: advmod\n",
      "missed - miss - POS: VERB, Tag: VERB, Dependency: ROOT\n",
      "Football - Football - POS: PROPN, Tag: PROPN, Dependency: compound\n",
      "season - season - POS: NOUN, Tag: NOUN, Dependency: dobj\n",
      "at - at - POS: ADP, Tag: ADP, Dependency: prep\n",
      "Notre - Notre - POS: PROPN, Tag: PROPN, Dependency: compound\n",
      "Dame - Dame - POS: PROPN, Tag: PROPN, Dependency: pobj\n",
      "London: GPE (Countries, cities, states)\n",
      "Europe: LOC (Non-GPE locations, mountain ranges, bodies of water)\n",
      "Notre Dame: FAC (Buildings, airports, highways, bridges, etc.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Text with typo (lving should be loving), semster changed to time and added comma after Europe\n",
    "text1 = \"Despite lving my time abroad in London and the opportunity to travel around Europe, I really missed Football season at Notre Dame\"\n",
    "doc1 = nlp(text1)\n",
    "\n",
    "for token in doc1:\n",
    "    print(f\"{token.text} - {token.lemma_} - POS: {token.pos_}, Tag: {token.pos_}, Dependency: {token.dep_}\")\n",
    "\n",
    "for ent in doc1.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Despite loving my semester abroad in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and the opportunity to travel around \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Europe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " I really missed Football season at \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Notre Dame\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altered Text:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Despite lving my time abroad in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and the opportunity to travel around \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Europe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", I really missed Football season at \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Notre Dame\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Comparison\n",
    "\n",
    "# Render the named entities of original text (doc)\n",
    "print(\"Original Text:\")\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n",
    "# Render the named entities of altered text (doc1)\n",
    "print(\"Altered Text:\")\n",
    "from spacy import displacy\n",
    "displacy.render(doc1, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "\n",
    "1. **How does spaCy handle your modifications?**\n",
    "- Despite having a typo in the word loving, the POS features are able to identify that it still is a prepositional complement dependency and tehrefore whould be followed by another sentence. The word chnage from semseter to time was also picked up and identified as a noun. Lastly, the pucntuation added was also documented by spaCy as a puntuation.\n",
    "\n",
    "2. **Did any entities or tags change? Why might this happen?**\n",
    "- Given that the changes made to the text didn't alter the menaing of the sentence, none of the tags or entities cheanged (as shown in the output above)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
