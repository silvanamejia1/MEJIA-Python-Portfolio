{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Installing the spaCy library\n",
    "!pip install spacy\n",
    "\n",
    "# Installing the small English language model used by spaCy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Tokenization\n",
    "### 1. Write a Python script to tokenize the following text: \n",
    "    \"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'does', \"n't\", 'jump', 'over', 'the', 'lazy', 'dog', '.', 'Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "text = (\"\"\"The quick brown fox doesn't jump over the lazy dog. Natural Language Processing is fascinating!\"\"\")\n",
    "\n",
    "import spacy\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the NLP pipeline to the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens\n",
    "tokens= [token.text for token in doc] #this puts it in a list \n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Questions to Answer in Comments:\n",
    "\n",
    "**1. How does spaCy process the various tokens?**\n",
    "    Hint: Loop through the doc container using the token attributes: .text_, .head, .lemma_, .morph.\n",
    "- The tokenization process  segments a sentence into words, punctuations, etc. A collection of tokens builds a doc. It processes each token by applying rules specific to each language. The first step is splitting the text based on white space (similar to split), and then the tokenizer performs two checks: 1. Are there any exceptions (ex: despite not having white space doesn't needs to be split), and 2. Can a prefix, suffix of infix be split off.\n",
    "\n",
    "**2. How does spaCy handle punctuation marks like periods and commas?**\n",
    "- Punctuation marks are treated as separte tokens.\n",
    "\n",
    "**3. What happens when the text includes contractions (e.g., \"don't\")?**\n",
    "- SpaCy splits splits tetx in such a way where each token is in its simplest form. For this reason contractions will be split into 2 words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Part-of-Speech Tagging\n",
    "### Extend your script to include part-of-speech tagging for the tokens.\n",
    "    Hint: Use token.pos_ and token.tag_ properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The - the: DET\n",
      "quick - quick: ADJ\n",
      "brown - brown: ADJ\n",
      "fox - fox: NOUN\n",
      "does - do: AUX\n",
      "n't - not: PART\n",
      "jump - jump: VERB\n",
      "over - over: ADP\n",
      "the - the: DET\n",
      "lazy - lazy: ADJ\n",
      "dog - dog: NOUN\n",
      ". - .: PUNCT\n",
      "Natural - Natural: PROPN\n",
      "Language - Language: PROPN\n",
      "Processing - processing: NOUN\n",
      "is - be: AUX\n",
      "fascinating - fascinating: ADJ\n",
      "! - !: PUNCT\n",
      "The - the - POS: DET, Tag: DT\n",
      "quick - quick - POS: ADJ, Tag: JJ\n",
      "brown - brown - POS: ADJ, Tag: JJ\n",
      "fox - fox - POS: NOUN, Tag: NN\n",
      "does - do - POS: AUX, Tag: VBZ\n",
      "n't - not - POS: PART, Tag: RB\n",
      "jump - jump - POS: VERB, Tag: VB\n",
      "over - over - POS: ADP, Tag: IN\n",
      "the - the - POS: DET, Tag: DT\n",
      "lazy - lazy - POS: ADJ, Tag: JJ\n",
      "dog - dog - POS: NOUN, Tag: NN\n",
      ". - . - POS: PUNCT, Tag: .\n",
      "Natural - Natural - POS: PROPN, Tag: NNP\n",
      "Language - Language - POS: PROPN, Tag: NNP\n",
      "Processing - processing - POS: NOUN, Tag: NN\n",
      "is - be - POS: AUX, Tag: VBZ\n",
      "fascinating - fascinating - POS: ADJ, Tag: JJ\n",
      "! - ! - POS: PUNCT, Tag: .\n"
     ]
    }
   ],
   "source": [
    "#WHICH TO USE\n",
    "#ask if the \"is.\" period is an error\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}: {token.pos_}\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_} - POS: {token.pos_}, Tag: {token.tag_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "1. **Identify the POS tags for \"quick,\" \"jumps,\" and \"is.\"**\n",
    "- \"quick\" POS tag: ADJ (adjective)\n",
    "- \"jumps\" POS tag: VERB\n",
    "- \"is\" POS tag: AUX (auxiliary verb)\n",
    "\n",
    "2. **Why might POS tagging be useful for tasks like grammar checking or machine translation?**\n",
    "- One of the main features of POS tagging is th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Named Entity Recognition (NER)\n",
    "### Modify your script to identify named entities in the following text:\n",
    "    \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama: PERSON (People, including fictional)\n",
      "44th: ORDINAL (\"first\", \"second\", etc.)\n",
      "the United States: GPE (Countries, cities, states)\n",
      "Hawaii: GPE (Countries, cities, states)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    44th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". He was born in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hawaii\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ASK WHY THE IS TAKEN AS A ENTITY\n",
    "#does q2 of the writtehn part need to be answer using code? tokens.properties\n",
    "text = (\"\"\"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\"\")\n",
    "\n",
    "#tokenize\n",
    "doc = nlp(text)\n",
    "tokens= [token.text for token in doc] #this puts it in a list \n",
    "\n",
    "# Display named entities in the text\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_} ({spacy.explain(ent.label_)})\")\n",
    "\n",
    "#Better Presentation using Displacy\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "\n",
    "1. **Which entities are recognized by spaCy?**\n",
    "   Hint: Loop through doc.ents \n",
    "- Barack Obama, 44th, the United States, Hawaii\n",
    "\n",
    "2. **What entity types are assigned to \"Barack Obama\" and \"Hawaii\"?**\n",
    "    Hint: Use token.label_ properties\n",
    "- Barack Obama: PERSON\n",
    "- Hawaii: GPE (Geopolitical entity: countries, cities, states, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Experimentation\n",
    "### Write a new sentence or paragraph of your choice and run the spaCy pipeline on it.\n",
    "### Experiment with changing words, adding punctuation, or introducing typos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite - POS: despite, Tag: SCONJ\n",
      "I - POS: I, Tag: PRON\n",
      "lovd - POS: lovd, Tag: VERB\n",
      "my - POS: my, Tag: PRON\n",
      "semester - POS: semester, Tag: NOUN\n",
      "abroad - POS: abroad, Tag: ADV\n",
      "in - POS: in, Tag: ADP\n",
      "London - POS: London, Tag: PROPN\n",
      "and - POS: and, Tag: CCONJ\n",
      "traveling - POS: travel, Tag: VERB\n",
      "around - POS: around, Tag: ADP\n",
      "Europe - POS: Europe, Tag: PROPN\n",
      ", - POS: ,, Tag: PUNCT\n",
      "I - POS: I, Tag: PRON\n",
      "really - POS: really, Tag: ADV\n",
      "missed - POS: miss, Tag: VERB\n",
      "Football - POS: Football, Tag: PROPN\n",
      "season - POS: season, Tag: NOUN\n",
      "at - POS: at, Tag: ADP\n",
      "Notre - POS: Notre, Tag: PROPN\n",
      "Dame - POS: Dame, Tag: PROPN\n",
      "Entity: London, Label: GPE\n",
      "Entity: Europe, Label: LOC\n",
      "Entity: Notre Dame, Label: FAC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Despite I lovd my semester abroad in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and traveling around \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Europe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", I really missed Football season at \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Notre Dame\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Text with typo (lovd should be loved)\n",
    "text = \"Despite I lovd my semester abroad in London and traveling around Europe, I really missed Football season at Notre Dame\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - POS: {token.lemma_}, Tag: {token.pos_}\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
    "\n",
    "#Better Presentation using Displacy\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent', jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer in Comments:\n",
    "\n",
    "1. **How does spaCy handle your modifications?**\n",
    "\n",
    "\n",
    "2. **Did any entities or tags change? Why might this happen?**\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
